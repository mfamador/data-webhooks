http:
  address: ${HTTP_ADDRESS:0.0.0.0:4195}

input:
  broker:
    inputs:
    # input from data topic
    - kafka_balanced:
        addresses:
        - ${KAFKA_ADDRESS:kafka:9093}
        tls:
          enabled: ${KAFKA_TLS_ENABLED:true}
        sasl:
          mechanism: ${KAFKA_SASL_MECHANISM}
          user: ${KAFKA_USER:"$ConnectionString"}
          password: ${KAFKA_PASSWORD:"Endpoint=sb://kafka/"}
        topics:
        - readings
        client_id:  ${KAFKA_CLIENT_ID:data_webooks}
        consumer_group: ${KAFKA_CONSUMER_GROUP:webhooks}
      processors:
      - catch: [ ]
    # input from data-retries topic - retry queue
    - kafka_balanced:
        addresses:
        - ${KAFKA_RETRIES_ADDRESS:kafka:9093}
        tls:
          enabled: ${KAFKA_TLS_ENABLED:true}
        topics:
        - readings-retries
        sasl:
          mechanism: ${KAFKA_SASL_MECHANISM}
          user: ${KAFKA_RETRIES_USER:"$ConnectionString"}
          password: ${KAFKA_RETRIES_PASSWORD}
        client_id: ${KAFKA_CLIENT_ID:data_webooks}
        consumer_group: ${KAFKA_CONSUMER_GROUP:webhooks}
      processors:
      - catch: [ ]
      - bloblang: |
          root = this.content
          meta sleep = timestamp - timestamp_unix()
          meta num_retries = num_retries
      - log:
          level: "INFO"
          message: "Sleeping for ${!metadata:sleep}"
      - sleep:
          duration: "${!metadata:sleep}s"

buffer:
  type: none

pipeline:
  processors:
  - switch:
    - check: keep=="alive"
      processors:
      - bloblang: |
          root = deleted()
  - type: matcher
    plugin:
      subscribers_service_address: ${SUBSCRIBERS_SERVICE_ADDRESS}

output:
  switch:
    retry_until_success: true # will retry until writes to an output
    # DLQ
    - check: 'meta("dead_letter")|"false" == "true" || (errored() && meta("num_retries").number()|env("MAX_RETRIES").number() <= 1)'
      output:
        resource: dead_letter
        processors:
        - log:
            level: ERROR
            message: "WRITING TO DEAD LETTER"   
        - bloblang: |
            root = match {
              meta("dead_letter")|"false" == "true" => this
              _ => this.pre_processed
            }
        - bloblang: |
            root = this
            samples = if this.samples.type() == "object" {
              [samples]
            }   
            
    # retry queue
    - check: 'errored() && meta("dead_letter")|"false" == "false" && meta("num_retries").number()|env("MAX_RETRIES").number() > 1'
      output:
        resource: retry_queue
        processors:
        # fix to write a single message
        - log:
            level: ERROR
            message: "WRITING TO RETRY BECAUSE: ${!error()}"      
        - bloblang: |
            content = this.pre_processed 
            timestamp = timestamp_unix() + env("RETRY_INTERVAL").number()|600
            num_retries = meta("num_retries").number()|env("MAX_RETRIES").number()|5 - 1
        - bloblang: |
            root = this
            content.samples = if this.content.samples.type() == "object" {
              [content.samples]
            }  

resources:
  outputs:
    dead_letter:
      blob_storage:
        storage_connection_string: ${AZURE_STORAGE_CONNECTION_STRING}
        container: 'dead-letter-readings-digestor-${!timestamp("2006")}'
        path: '${!timestamp("2006-01-02")}/${!timestamp_unix_nano()}.json'
    retry_queue:
      kafka:
        addresses:
        - ${KAFKA_RETRIES_ADDRESS:kafka:9093}
        tls:
          enabled: ${KAFKA_TLS_ENABLED:true}
        topic: readings-retries
        sasl:
          mechanism: ${KAFKA_SASL_MECHANISM}
          user: ${KAFKA_RETRIES_USER:"$ConnectionString"}
          password: ${KAFKA_RETRIES_PASSWORD}
        client_id: ${KAFKA_CLIENT_ID:readings_digestor}

metrics:
  prometheus:
    prefix: data_webhooks

logger:
  prefix: data_webhooks
  level: ${LOGLEVEL:INFO}
  add_timestamp: true
  json_format: true
  static_fields:
    '@service': data_webhooks

tracer:
  jaeger:
    agent_address: jaeger-agent.istio-system:6831
    service_name: data-webhooks
    sampler_type: const
    flush_interval: ""
